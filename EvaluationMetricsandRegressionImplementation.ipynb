{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530335b-c933-442f-9b62-59923c8d055f",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does R-squared represent in a regression model\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used in regression analysis to assess the goodness of fit of a regression model. It quantifies the proportion of the variance in the dependent variable that is predictable from the independent variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c11bbc-1fd2-487c-a111-8b9a4985a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the assumptions of linear regression\n",
    "linearity\n",
    "independence of residuals,\n",
    " homoscedasticity\n",
    "normality of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198a065-bcbb-4a1c-8109-74a33d192e23",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the difference between R-squared and Adjusted R-squared\n",
    "R-squared measures the proportion of variance in the dependent variable explained by the independent variables. It always increases when more predictors are added, regardless of their relevance to the model.\n",
    "Adjusted R-squared, on the other hand, adjusts the R-squared value based on the number of predictors in the model. It accounts for the degrees of freedom, penalizing the addition of irrelevant predictors. As a result, Adjusted R-squared can decrease if a new predictor does not improve the model sufficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e68af-dc3e-46a2-bf08-8c120482df50",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why do we use Mean Squared Error (MSE)\n",
    "Mean Squared Error (MSE) is a fundamental metric in statistics and machine learning used to measure the accuracy of predictive models. It quantifies the average squared difference between the predicted values and the actual values in a dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ccbfd-93c5-44a1-92c8-b757a376558d",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does an Adjusted R-squared value of 0.85 indicate\n",
    "an R-squared value of 0.85 suggests that 85% of the variance in the dependent variable can be explained by the independent variables in the model. R-squared = 0.0: The predicted points are randomly scattered, indicating the model has no predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1016fc2-efed-43dc-9a3c-252e319cc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " How do we check for normality of residuals in linear regression\n",
    "A normal quantile-quantile (Q-Q) plot of the residuals. It is used to assess the normality of the residuals.\n",
    "If the points in this plot closely follow a straight line, it indicates that the residuals are approximately normally distributed, which is an important assumption in linear regression.\n",
    "Deviations from a straight line may suggest departures from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a3ad5-c116-4a82-bba5-b326112c3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is multicollinearity, and how does it impact regression\n",
    "Causes of Multicollinearity\n",
    "\n",
    "Multicollinearity can arise due to several reasons:\n",
    "\n",
    "Correlation Among Predictor Variables: When predictor variables exhibit high correlation, one predictor can be accurately predicted from the others\n",
    "1\n",
    ".\n",
    "\n",
    "Overparameterization: Introducing too many predictor variables relative to the number of observations can lead to redundancy and increased variance of coefficient estimates\n",
    "1\n",
    ".\n",
    "\n",
    "Data Collection Issues: Problems in data collection, such as measuring variables with exceptional precision or inherent interconnections, can introduce multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf8160-d99a-4744-88e7-2b5877eaf905",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Mean Absolute Error (MAE)\n",
    "The Mean Absolute Error (MAE) is the average of all absolute errors. The formula is: |x i – x| = the absolute errors. The formula may look a little daunting, but the steps are easy: Find all of your absolute errors, x i – x. Add them all up. Divide by the number of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0fa72-5081-4663-8812-ce56ee83617a",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the benefits of using an ML pipeline\n",
    "Makes building models more efficient and simplified\n",
    "Helps cut redundant work\n",
    "Moves the product from just the model to the pipeline/workflow and this improves efficiency and scalability\n",
    "Easy to monitor each components/stage\n",
    "Fast iteration cycle\n",
    "Reduces the chance of error and saves time by automating repetitive tasks\n",
    "Allows you to monitor and tune processes\n",
    "Improves the quality of your ML predictions\n",
    "Decreases the risk of manual error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e892d-b67f-49c6-a3c5-d7f9742620b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is RMSE considered more interpretable than MSE\n",
    "RMSE is in the same units as the target variable being predicted, while MSE is in squared units. This makes RMSE more interpretable. Similarly, RMSE is scale-dependent, meaning it is related to the scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59b9db-c35e-4d2c-a217-fc574abc3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is pickling in Python, and how is it useful in ML\n",
    "Pickling is a way to convert a Python object (list, dictionary, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another Python script. It provides a facility to convert any Python object to a byte stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfd859-2975-442c-8834-cadf9257a119",
   "metadata": {},
   "outputs": [],
   "source": [
    " What does a high R-squared value mean\n",
    "High R-squared value means strong agreement between regression model and actual observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930bcde-2c18-46fb-9b98-15e1c247b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    " What happens if linear regression assumptions are violated\n",
    "If one or more than of the assumption of linear regression are violated the result of analysis may be incorrect or misleading.Violation of the assumption\n",
    "can be  lead to biased regression coeffcient ,biased standard error,biased  p-value,biased R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff31173-84e8-4df5-b39a-95aa4d08b119",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can we address multicollinearity in regression\n",
    "Causes of Multicollinearity\n",
    "\n",
    "Multicollinearity can arise due to several reasons:\n",
    "\n",
    "Correlation Among Predictor Variables: When predictor variables exhibit high correlation, one predictor can be accurately predicted from the others\n",
    "1\n",
    ".\n",
    "\n",
    "Overparameterization: Introducing too many predictor variables relative to the number of observations can lead to redundancy and increased variance of coefficient estimates\n",
    "1\n",
    ".\n",
    "\n",
    "Data Collection Issues: Problems in data collection, such as measuring variables with exceptional precision or inherent interconnections, can introduce multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf440b6-7dc9-41d3-8593-bfd5aeb2e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can feature selection improve model performance in regression analysis\n",
    "Reduce model complexity: Fewer features mean a simpler model, which is easier to interpret and faster to train.\n",
    "Improve model performance: Removing irrelevant features can enhance the model's predictive accuracy.\n",
    "Prevent overfitting: With fewer features, the model is less likely to learn noise from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580e3f7-7dba-4c5f-8b55-d289921768f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " How is Adjusted R-squared calculated\n",
    "o calculate the adjusted R-squared, follow these steps:\n",
    "Calculate the regular R-squared value.\n",
    "Use the formula: Adjusted R2 = 1 – [(1-R2) * (n-1) / (n-k-1)], where R2 is the R-squared of the model, n is the number of observations, and k is the number of predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ff0e-c8d4-434b-9334-578faa1c64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is MSE sensitive to outliers\n",
    "The MSE measures the average of the squared differences between predicted values and actual target values. By squaring the differences, the MSE places a higher weight on larger errors, making it sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960a23d-b719-438b-b0a1-e352d0fe4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the role of homoscedasticity in linear regression\n",
    "Linear regression is one of the most used and simplest algorithms in machine learning, which helps predict linear data in almost all kinds of problem statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4040a7-adba-47bc-90c9-b8b4bf01e303",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Root Mean Squared Error (RMSE)\n",
    "he Root Mean Square Error (RMSE) is a commonly used metric to measure the differences between predicted values and observed values. It is the square root of the average of squared differences between the predicted and observed values. The formula for RMSE is:\n",
    "\n",
    "RMSE = √Σ (Pi – Oi)&#178; / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febc692-bd38-4166-bd2a-d62321a371dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " Why is pickling considered risky\n",
    "Molds growing in pickles can raise the pH. A raised pH increases the chance that harmful organisms that cause botulism grow. Botulinum toxins block nerve functions and can lead to respiratory and muscular paralysis. Fresh packing of cucumbers creates small wastewater loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c48eeb-7c0a-45c9-84e7-b1617d8f71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " How can interaction terms enhance a regression model's predictive power?\n",
    "Thus, adding interaction terms as variables to a regression model allows to acknowledge these intersections and, therefore, enhance the model’s fitness for purpose in terms of explaining the patterns underlying the observed data and/or predicting future values of the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548849-d40e-422a-8e5d-c7f641089dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model \n",
    "using Seaborn's \"diamonds\" dataset\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load diamonds dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Drop rows with missing values (if any)\n",
    "diamonds.dropna(inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=[\"int64\", \"floa]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8fcef-a00a-47a0-8313-859a606ec8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root \n",
    "Mean Squared Error (RMSE) for a linear regression model\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the diamonds dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "diamonds.dropna(inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Preprocessing: One-hot encoding for categorical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline: Preprocessing + Linear Regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Squared Error (MS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b034854-9b32-4761-9c6f-e97d796a2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check \n",
    "linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a416e4-2f05-4cef-9cf0-c2a55a8acf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the \n",
    "performance of different regression models\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\").dropna()\n",
    "\n",
    "# Features and target\n",
    "X = diamonds.drop(\"p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621e387-3f1b-47cf-80c4-814ef3e138d6",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and \n",
    "R-squared score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load dataset\n",
    "diamonds = sns.load_dataset(\"diamonds\").dropna()\n",
    "\n",
    "# Simple Linear Regression: Use 'carat' to predict 'price'\n",
    "X = diamonds[[\"carat\"]]  # Single feature in 2D\n",
    "y = diamonds[\"price\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Get metrics\n",
    "coef = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Simple Linear Regression Model:\")\n",
    "print(f\"Coefficient (slope): {coef:.2f}\")\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd352e-f6b4-4f13-9dbc-5243f1e46e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using \n",
    "simple linear regression and visualizes the results\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Feature and target\n",
    "X = tips[[\"total_bill\"]]  # Predictor must be 2D\n",
    "y = tips[\"tip\"]\n",
    "\n",
    "# Train/test split (optional, but good practice)\n",
    "X_train, X_test, y_train,_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bba19f-9ef5-4e8d-8ef6-ef169ad7e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the \n",
    "model to predict new values and plot the data points along with the regression line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)  # 100 p*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9535a3b-40c1-466f-81c1-65d08e88e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    " 8. Write a Python script that pickles a trained linear regression model and saves it to a file.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# 2. Train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Pickle the model to a file\n",
    "filena\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4a711-3a5d-47ac-882f-3a1a83ac528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the \n",
    "regression curve.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 3 + 2 * X + X**2 + np.random.randn(100, 1)  # Quadratic relationship\n",
    "\n",
    "# 2. Create a pipeline for polynomial regression (degree 2)\n",
    "degree = 2\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Generate prediction curve\n",
    "X_plot = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "y_pred = model.predict(X_plot)\n",
    "\n",
    "# 4. Plot the data and regression curve\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.plot(X_plo_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729d3e0-ab57-4816-bc61-a352ffeb2489",
   "metadata": {},
   "outputs": [],
   "source": [
    " Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear \n",
    "regression model to the data. Print the model's coefficient and intercept\n",
    "                        import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = 10 * np.random.rand(100, 1)  # Feature values between 0 and 10\n",
    "y = 5 + 2 * X + np.random.randn(100, 1)  # y = 5 + 2x + noise\n",
    "\n",
    "# 2. Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Print the model's parameters\n",
    "print(f\"Coefficient (slope): {model.coef_[0][0]:.2f}\")\n",
    "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aff62b-185f-4312-afd5-17025873094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and \n",
    "compares their performance.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Generate synthetic data with a nonlinear relationship\n",
    "np.random.seed(42)\n",
    "X = 6 * np.random.rand(100, 1) - 3  # Feature values between -3 and 3\n",
    "y = 0.5 * X**3 - X**2 + 2 + np.random.randn(100, 1)  # Cubic relationship + noise\n",
    "\n",
    "# 2. Define degrees to test\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 3. Prepare plot\n",
    "plt.scatter(X, y, color='black', label='Data', alpha=0.5)\n",
    "X_plot = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "\n",
    "# 4. Fit models, calculate MSE and plot results\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(f\"Degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dd57f-e753-4f12-8e95-122bb22285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Write a Python script that fits a simple linear regression model with two features and prints the model's \n",
    "coefficients, intercept, and R-squared score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2) * 10  # 100 samples, 2 features (values between 0 and 10)\n",
    "\n",
    "# Define a linear relationship with some noise: y = 3 + 2*x1 + 4*x2 + noise\n",
    "y = 3 + 2 * X[:, 0] + 4 * X[:, 1] + np.random.randn(100) * 1.5\n",
    "\n",
    "# 2. Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Predict on training data to calculate R-squared\n",
    "y_pred = model.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# 4. Print results\n",
    "print(\"Coefficients:\", model.coef_)          # array with 2 coefficients\n",
    "print(\"Intercept:\", mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6c4fd-48a0-4f18-8b76-4b7a8b00ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the \n",
    "regression line along with the data points.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)  # 100 samples between 0 and 2\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
    "\n",
    "# 2. Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Generate predictions for regression line\n",
    "X_line = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "y_line = model.predict(X_line)\n",
    "\n",
    "# 4. Plot data points and regression line\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X_line, y_line, color='red', label='Regress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62bf3e-8d77-40c5-9cda-7d2ea9510ced",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset \n",
    "with multiple features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 1. Generate synthetic dataset with multicollinearity\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create independent variables\n",
    "X1 = np.random.normal(0, 1, n_samples)\n",
    "X2 = 0.8 * X1 + np.random.normal(0, 0.2, n_samples)  # Highly correlated with X1\n",
    "X3 = np.random.normal(0, 1, n_samples)\n",
    "X4 = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Put data into a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'X1': X1,\n",
    "    'X2': X2,\n",
    "    'X3': X3,\n",
    "    'X4': X4\n",
    "})\n",
    "\n",
    "# 2. Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = data.columns\n",
    "vif_data['VIF'] = [variance_infla_]()_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6bec5-0055-4caf-9aed-8f4e0e8cd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a \n",
    "polynomial regression model, and plots the regression curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Generate synthetic data with degree 4 polynomial relationship\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "# y = 1 - 2x + 0.5x^2 - x^3 + 0.3x^4 + noise\n",
    "y = 1 - 2*X + 0.5*X**2 - 1*X**3 + 0.3*X**4 + np.random.randn(100, 1) * 3\n",
    "\n",
    "# 2. Create polynomial regression model (degree 4)\n",
    "degree = 4\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Predict over a smooth range for plotting\n",
    "X_plot = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "y_plot = model.predict(X_plot)\n",
    "\n",
    "# 4. Plot data points and regression curve\n",
    "plt.scatter(X, y, color='blue', alpha=0.6, label='Data points')\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label=f'Polynomial Regression (Degree {degree})')\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "plt.title('Polynomial Regression (Degree 4)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93490c57-10d1-4d9a-8a47-4149c78b48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script that creates a machine learning pipeline with data standardization and a multiple \n",
    "linear regression model, and prints the R-squared score\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate synthetic data with multiple features\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3) * 10  # 100 samples, 3 features\n",
    "# Define target with a linear relationship + noise\n",
    "y = 3 + 2 * X[:, 0] + 4 * X[:, 1] + 5 * X[:, 2] + np.random.randn(100) * 2\n",
    "\n",
    "# 2. Create pipeline: StandardScaler + LinearRegression\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# 3. Fit pipeline on data\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 4. Predict and calculate R-squared score\n",
    "y_pred = pipeline.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"R-squared score on training data: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e691d58-759b-4b0c-8d3b-e025d5236c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the \n",
    "regression curve.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "# Cubic relationship with noise\n",
    "y = 1 + 2*X - 0.5*X**2 + 0.3*X**3 + np.random.randn(100, 1) * 2\n",
    "\n",
    "# 2. Create polynomial regression model (degree 3)\n",
    "degree = 3\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Generate predictions for plotting\n",
    "X_plot = np.linspace(-3, 3, 200).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaf2a4-b34b-45e2-a3e9-8c03e3aadc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print \n",
    "the R-squared score and model coefficients.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "X = np.random.rand(n_samples, n_features) * 10  # Features between 0 and 10\n",
    "\n",
    "# True coefficients for each feature\n",
    "true_coefs = np.array([1.5, -2.0, 3.0, 0.0, 4.5])\n",
    "\n",
    "# Generate target variable with noise\n",
    "y = X @ true_coefs + 5 + np.random.randn(n_samples) * 2  # y = X*coefs + intercept + noise\n",
    "\n",
    "# 2. Fit multiple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Predict and compute R-squared\n",
    "y_pred = model.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"R-squared score: {r2:.4f}\")\n",
    "print(f\"Model co\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314b44d-a091-4412-a72d-43a76e6028f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the \n",
    "data points along with the regression line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)  # 100 samples between 0 and 2\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
    "\n",
    "# 2. Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Generate predictions for regression line\n",
    "X_line = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "y_line = model.predict(X_line)\n",
    "\n",
    "# 4. Plot data points and regression line\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X_line, y_line, color='red', label='Regression line', linewidth=2)\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression: Data and Regression Line')\n",
    "plt.lege\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb0d35-28aa-4bbd-b2fa-fa43380e68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R\n",
    "squared score and coefficient\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "\n",
    "X = np.random.rand(n_samples, n_features) * 10  # Features between 0 and 10\n",
    "\n",
    "# True coefficients for each feature\n",
    "true_coefs = np.array([1.5, -2.0, 3.0])\n",
    "\n",
    "# Generate target variable with noise\n",
    "y = X @ true_coefs + 5 + np.random.randn(n_samples) * 2  # y = X*coefs + intercept + noise\n",
    "\n",
    "# 2. Fit multiple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Predict and compute R-squared\n",
    "y_pred = model.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"R-squared score: {r2:.4f}\")\n",
    "print(f\"Model coefficients: {model.coef_}\")\n",
    "print(f\"Model intercept: {model.intercept_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db187b0-c074-4788-9049-f334602817d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write a Python script that demonstrates how to serialize and deserialize machine learning models using \n",
    "joblib instead of pickling.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([3, 6, 9, 12, 15])  # Perfect linear relation y=3x\n",
    "\n",
    "# 2. Train a simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Serialize (save) the model to a file\n",
    "joblib_file = 'linear_model.joblib'\n",
    "joblib.dump(model, joblib_file)\n",
    "print(f\"Model saved to {joblib_file}\")\n",
    "\n",
    "# 4. Deserialize (load) the model from the file\n",
    "loaded_model = joblib.load(joblib_file)\n",
    "print(\"Model loaded from file.\")\n",
    "\n",
    "# 5. Use loaded model for prediction\n",
    "X_new = np.array([[6], [7]])\n",
    "predictions = loaded_model.predict(X_new)\n",
    "print(f\"P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d71f66-facc-4a79-9cd9-6557a8538e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python script to perform linear regression with categorical features using one-hot encoding. Use \n",
    "the Seaborn 'tips' dataset\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Load the tips dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# 2. Select features and target\n",
    "# Features: total_bill (numerical), sex, smoker, day, time (categorical)\n",
    "X = tips[['total_bill', 'sex', 'smoker', 'day', 'time']]\n",
    "y = tips['tip']\n",
    "\n",
    "# 3. One-hot encode categorical features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_spli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beec24-013a-4ab1-8e15-86ed379b0931",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R\n",
    "squared score.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coefs = np.array([1.5, -2.0, 3.0, 0.0, 4.5])\n",
    "y = X @ true_coefs + 5 + np.random.randn(n_samples) * 1.5  # y = X*coefs + intercept + noise\n",
    "\n",
    "# 2. Fit Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_pred_lin = lin_reg.predict(X)\n",
    "r2_lin = r2_score(y, y_pred_lin)\n",
    "\n",
    "# 3. Fit Ridge Regression (with alpha=1.0)\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fi_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dac8b-2975-43ce-ba64-bbe283c10334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8007f-9693-4d74-bcd7-5600db5a9858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
